{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPUxEKyhRHpz5GiB+a/XHes",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saravanamuthu1/sentiment_analysis/blob/main/sentiment_analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "y3slKIqK6tfr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from collections import Counter\n",
        "import math\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "v4KMA4l736vp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Naive_bayes:\n",
        "  def __init__(self) -> None:\n",
        "    self.mylist=[]\n",
        "    self.li=[]\n",
        "    self.V_freq = {}\n",
        "    self.freq_pos={}\n",
        "    self.freq_neg={}\n",
        "    self.df=None\n",
        "    self.total_porobaility_for_pos=0\n",
        "    self.total_porobaility_for_neg=0\n",
        "    self.liklywood_pos={}\n",
        "    self.liklywood_neg={}\n",
        "  def get_data_set(self,my_data):\n",
        "    for line in my_data:\n",
        "      value=line.rstrip(\"\\n\").split(\"\\t\")\n",
        "      self.mylist.append(line.rstrip(\"\\n\").split(\"\\t\"))\n",
        "    df1=pd.DataFrame(self.mylist)\n",
        "    df1.columns=[\"label\",\"tweet\"]\n",
        "    return df1,self.mylist\n",
        "  #fit function\n",
        "  def fit(self,df1):\n",
        "    df1['train_value'] = df1.train_value.apply(lambda x:re.sub(r'[^\\w\\s]', '', x))\n",
        "    df1['train_value'] = df1.train_value.apply(lambda x:x.strip())\n",
        "    df1['train_value'] =df1.train_value.apply(lambda x:x.replace(\"USER\", \"\"))\n",
        "    df1[\"train_value\"]= df1.train_value.apply(lambda x:\" \".join(x.split()))\n",
        "    df1[\"train_value\"]= df1.train_value.apply(lambda x:''.join([i for i in x if not i.isdigit()]))\n",
        "    self.freq_pos= Counter(\" \".join(df1.loc[df1.train_label=='1']['train_value'].values.tolist()).split(\" \"))\n",
        "    self.V_freq=Counter(\" \".join(df1['train_value'].values.tolist()).split(\" \"))\n",
        "    self.freq_neg = Counter(\" \".join(df1.loc[df1.train_label=='0']['train_value'].values.tolist()).split(\" \")) \n",
        "    self.total_porobaility_for_pos=np.log(len(self.freq_pos)-len(self.freq_neg))\n",
        "    self.total_porobaility_for_neg=np.log(len(self.freq_pos)-len(self.freq_neg))\n",
        "    for w in self.V_freq:\n",
        "      if w in self.freq_pos:\n",
        "        p_w_pos_value=float((1+(self.freq_pos.get(w,0)))/(len(self.V_freq)+len(self.freq_pos)))\n",
        "      if w in self.freq_neg:\n",
        "        p_w_neg_value = float((1+(self.freq_neg.get(w,0)))/(len(self.V_freq)+len(self.freq_neg)))\n",
        "      self.liklywood_pos[w] = p_w_pos_value\n",
        "      self.liklywood_neg[w] = p_w_neg_value\n",
        "\n",
        "  # data pre-processing\n",
        "  def pre_processing(self,mytest_file):\n",
        "    mytest_file['train_value'] = mytest_file.train_value.apply(lambda x:re.sub(r'[^\\w\\s]', '', x))\n",
        "    mytest_file['train_value'] = mytest_file.train_value.apply(lambda x:x.strip())\n",
        "    mytest_file['train_value'] = mytest_file.train_value.apply(lambda x:x.replace(\"USER\", \"\"))\n",
        "    mytest_file[\"train_value\"]= mytest_file.train_value.apply(lambda x:\" \".join(x.split()))\n",
        "    mytest_file[\"train_value\"]= mytest_file.train_value.apply(lambda x:''.join([i for i in x if not i.isdigit()]))\n",
        "    return mytest_file\n",
        "  # predict fucntion\n",
        "  def predict(self,mytxt):\n",
        "    p1=0.5\n",
        "    p2=0.5\n",
        "    mytxt=mytxt.split()\n",
        "    mytxt=list(set(mytxt))\n",
        "    pos_score = self.total_porobaility_for_pos\n",
        "    neg_score = self.total_porobaility_for_neg\n",
        "    p=self.total_porobaility_for_pos\n",
        "    for w in mytxt:\n",
        "      if w in self.liklywood_pos:\n",
        "        p1+=self.liklywood_pos[w]\n",
        "      if w in self.liklywood_neg:\n",
        "        p2+=self.liklywood_neg[w]\n",
        "    total_value=max(p1,p2)\n",
        "    if total_value == p1:\n",
        "      return 1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  #to fit the data\n",
        "  testfile = open('train-v2.tsv', 'r')\n",
        "  data_value=[]\n",
        "\n",
        "  obj1= Naive_bayes()\n",
        "  df1,total_data=obj1.get_data_set(testfile)\n",
        "  # predict values\n",
        "  # upload your dataset in the colab and change the name here to predict\n",
        "  predict_file= open('train-v2.tsv', 'r')\n",
        "  #since we don't have test set we have used split the entire corpus. we can alter still alter the program for jsut test set alone.\n",
        "  train_set=total_data[0:80000]\n",
        "  df2=pd.DataFrame(train_set)\n",
        "  df2.columns=[\"train_label\",\"train_value\"]\n",
        "  df2=obj1.pre_processing(df2)\n",
        "  obj1.fit(df2)\n",
        "  df2['train_label'] = df2.train_label.apply(lambda x:int(x))\n",
        "  testfile1 = open('test.tsv', 'r')\n",
        "  for line in testfile1:\n",
        "      value1=line.rstrip(\"\\n\").split(\"\\t\")\n",
        "      data_value.append(value1)\n",
        "  df3 =pd.DataFrame(data_value)\n",
        "  df3.columns=[\"test_label\",\"test_value\"]\n",
        "  df3['test_label'] = df3.test_label.apply(lambda x:int(x))\n",
        "  df2['total_prediction_train']=df2.train_value.apply(lambda x:obj1.predict(x))\n",
        "  df3['total_prediction_test']=df3.test_value.apply(lambda x:obj1.predict(x))\n",
        "  accuracy1 = accuracy_score(df2['train_label'], df2['total_prediction_train'])\n",
        "  accuracy2 = accuracy_score(df3['test_label'], df3['total_prediction_test'])\n",
        "  print(\"accuracy on the trianing_set:\",accuracy1)\n",
        "  print(\"accuracy on the test_set:\",accuracy2)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "\n",
        "    \n",
        "\n",
        "\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7exj7cWZSCMy",
        "outputId": "2d3da594-62de-41a3-f02d-04ff85e8f63d"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy on the trianing_set: 0.600825\n",
            "accuracy on the test_set: 0.5991\n"
          ]
        }
      ]
    }
  ]
}